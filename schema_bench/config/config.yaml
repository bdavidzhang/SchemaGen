llm:
  provider: "openai" # Options: openai
  model: "gpt-4-turbo"
  temperature: 0.1
  max_retries: 3
  api_key_env_var: "OPENAI_API_KEY"

evaluation:
  output_dir: "results"
  max_scenarios: null # Set to a number to limit test run
  save_responses: true # Save raw LLM responses for debugging

reporting:
  format: "csv" # Options: csv, json
